{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Full name**: Dang Ha Nguyen (Janice) Nguyen\n",
    "\n",
    "**ID**: 47856491\n",
    "\n",
    "**Repository**: [GitHub Repository](https://github.com/JaniceNguyen/BUSA8001_Assignment1)\n",
    "\n",
    "# BUSA8001 - APPLIED PREDICTIVE ANALYTICS\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Introduction**\n",
    "1. **Project Overview**:  \n",
    "This project involves analyzing a dataset provided by a financial institution to understand the credit behavior of its clients. The analysis is based on two primary datasets: `application_record.csv` and `credit_record.csv`. The `application_record.csv` dataset contains detailed information about the clients, such as their income, education, employment, and demographic details. The `credit_record.csv` dataset provides a monthly record of the clients' credit status, indicating whether they have missed payments or are overdue.\n",
    "\n",
    "The objective of this project is to preprocess the data, explore it, and develop predictive models that can help identify clients who are at risk of defaulting on their loans. By leveraging the insights gained from these models, the financial institution can better manage its credit risk and make more informed lending decisions.\n",
    "\n",
    "2. **Problem Statement**:  \n",
    "The specific problem this project aims to solve is the identification of clients who are likely to default on their loans. Defaulting clients pose a significant risk to financial institutions, and being able to predict such behavior allows the institution to take proactive measures. The goal is to use the historical application and credit data to build a model that can accurately predict whether a client will default based on their past credit records and application data.\n",
    "\n",
    "3. **Data Description**:  \n",
    "For this assignment there are two files in the `data` folder `credit_record.csv` and `application_record.csv` where bank clients are related by the `ID` column.\n",
    "\n",
    "In `application_record.csv` we have the following variables\n",
    "\n",
    "| Feature Name         | Explanation     | Additional Remarks |\n",
    "|--------------|-----------|-----------|\n",
    "| ID | Randomly allocated client number      |         |\n",
    "| AMT_INCOME   | Annual income  |  |\n",
    "| NAME_INCOME_TYPE   | Income Source |  |\n",
    "| NAME_EDUCATION_TYPE   | Level of Education  |  |\n",
    "| CODE_GENDER   | Applicant's Gender   |  |\n",
    "| FLAG_OWN_CAR | Car Ownership |  | \n",
    "| CNT_CHILDREN | Number of Children | |\n",
    "| FLAG_OWN_REALTY | Real Estate Ownership | | \n",
    "| NAME_FAMILY_STATUS | Relationship Status | | \n",
    "| NAME_HOUSING_TYPE | Housing Type | | \n",
    "| DAYS_BIRTH | No. of Days | Count backwards from current day (0), -1 means yesterday\n",
    "| DAYS_EMPLOYED | No. of Days | Count backwards from current day (0). If positive, it means the person is currently unemployed.\n",
    "| FLAG_MOBIL | Mobile Phone Ownership | | \n",
    "| FLAG_WORK_PHONE | Work Phone Ownership | | \n",
    "| FLAG_PHONE | Landline Phone Ownership | | \n",
    "| FLAG_EMAIL | Landline Phone Ownership | | \n",
    "| OCCUPATION_TYPE | Occupation | | \n",
    "| CNT_FAM_MEMBERS | Count of Family Members | |\n",
    "\n",
    "\n",
    "\n",
    "In `credit_record.csv` we have the following variables\n",
    "\n",
    "\n",
    "| Feature Name         | Explanation     | Additional Remarks |\n",
    "|--------------|-----------|-----------|\n",
    "| ID | Randomly allocated client number | |\n",
    "| MONTHS_BALANCE | Number of months in the past from now when STATUS is measured | 0 = current month, -1 = last month, -2 = two months ago, etc.|\n",
    "| STATUS | Number of days a payment is past due | 0: 1-29 days past due 1: 30-59 days past due 2: 60-89 days overdue 3: 90-119 days overdue 4: 120-149 days overdue 5: Overdue or bad debts, write-offs for more than 150 days C: paid off that month X: No loan for the month |\n",
    "\n",
    "The two datasets are linked by the `ID` column, which represents the unique identifier for each client. By merging these datasets, we can analyze how the characteristics captured in `application_record.csv` correlate with the credit behavior recorded in `credit_record.csv`. This combined analysis will be crucial for building models to predict loan defaults.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Task 1: Reading, Summarizing, and Cleaning Data**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Question 1**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **Loading Data**:\n",
    "   - Import the datasets using pandas into `df_application` and `df_credit`.\n",
    "   - Check the number of rows in each dataset.\n",
    "   - Determine the number of unique clients in each dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import data from csv file and store it in a DataFrame \n",
    "df_application = pd.read_csv('data/application_record.csv')\n",
    "df_credit = pd.read_csv('data/credit_record.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows in df_application: 438445\n",
      "Number of rows in df_credit: 1047185\n"
     ]
    }
   ],
   "source": [
    "# Describe the data\n",
    "## Number of rows and columns in each DataFrame\n",
    "print(f\"Number of rows in df_application: {df_application.shape[0]}\")\n",
    "print(f\"Number of rows in df_credit: {df_credit.shape[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique clients in df_application: 438398\n",
      "Number of unique clients in df_credit: 45924\n"
     ]
    }
   ],
   "source": [
    "## Number of unique values in each DataFrame\n",
    "print(f\"Number of unique clients in df_application: {df_application['ID'].nunique()}\")\n",
    "print(f\"Number of unique clients in df_credit: {df_credit['ID'].nunique()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. **Data Merging**:\n",
    "   - Merge `df_application` and `df_credit` on the `ID` column to create a combined dataset `df`.\n",
    "   - Evaluate the combined dataset by checking the number of rows and unique clients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge the DataFrames\n",
    "df = pd.merge(df_application, df_credit, on='ID', how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows in df: 776325\n",
      "Number of unique clients in df: 36396\n"
     ]
    }
   ],
   "source": [
    "# Describe the data\n",
    "## Number of rows and columns in DataFrame\n",
    "print(f\"Number of rows in df: {df.shape[0]}\")\n",
    "## Number of unique values in each DataFrame\n",
    "print(f\"Number of unique clients in df: {df['ID'].nunique()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The fact that there are 776,325 rows in `df` but only 36,396 unique clients suggests that each client (`ID`) is represented by multiple rows. This aligns with our understanding that the `credit_record.csv` file contains multiple records per client, corresponding to different months of credit activity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. **Exploratory Data Analysis (EDA)**:\n",
    "   - Describe how the merged data contains multiple rows per `ID` and what differentiates these rows."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To demonstrate how multiple rows for each `ID` in the merged dataset (`df`) are different, we can use code to show the differences between these rows. Specifically, we'll focus on the `MONTHS_BALANCE` and `STATUS` columns, which distinguish the rows for each client."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selecting the first ID for demonstration\n",
    "temp_id = df['ID'].iloc[0]\n",
    "# Filter the DataFrame to show all records for this ID\n",
    "df_filtered = df[df['ID'] == temp_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         ID  MONTHS_BALANCE STATUS\n",
      "0   5008804               0      C\n",
      "1   5008804              -1      C\n",
      "2   5008804              -2      C\n",
      "3   5008804              -3      C\n",
      "4   5008804              -4      C\n",
      "5   5008804              -5      C\n",
      "6   5008804              -6      C\n",
      "7   5008804              -7      C\n",
      "8   5008804              -8      C\n",
      "9   5008804              -9      C\n",
      "10  5008804             -10      C\n",
      "11  5008804             -11      C\n",
      "12  5008804             -12      C\n",
      "13  5008804             -13      1\n",
      "14  5008804             -14      0\n",
      "15  5008804             -15      X\n"
     ]
    }
   ],
   "source": [
    "# Display the filtered data\n",
    "df_filtered_sorted = df_filtered.sort_values(by='MONTHS_BALANCE', ascending=False)\n",
    "print(df_filtered_sorted[['ID', 'MONTHS_BALANCE', 'STATUS']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output of this section shows multiple rows for the selected `ID` (temporary ID for demonstration), where each row represents a different month's credit record. The `MONTHS_BALANCE` column indicates the number of months in the past from the current month, while the `STATUS` column shows the credit status for that month.\n",
    "\n",
    "This proves that the multiple rows for each `ID` are different because they capture the client's credit behavior over different months."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Question 2**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **Change the Values of `STATUS`**\n",
    "\n",
    "We need to map the values of the `STATUS` column in `df` according to the given rules:\n",
    "\n",
    "- Map `{C, X, 0}` to `0`\n",
    "- Map `{1, 2, 3, 4, 5}` to `1`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map the STATUS values\n",
    "df['STATUS'] = df['STATUS'].replace({'C': '0', 'X': '0', '0': '0', '1': '1', '2': '1', '3': '1', '4': '1', '5': '1'})\n",
    "\n",
    "# Ensure that STATUS is of integer type\n",
    "df['STATUS'] = df['STATUS'].astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. **Create `list_of_past_due`**\n",
    "\n",
    "To identify clients with `STATUS = 1` at any point in the last 12 months, we can filter the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter for records in the last 12 months\n",
    "df_last_12_months = df[df['MONTHS_BALANCE'] >= -12]\n",
    "\n",
    "# Find IDs with STATUS = 1\n",
    "list_of_past_due = df_last_12_months[df_last_12_months['STATUS'] == 1]['ID'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. **Create `df_final`**\n",
    "\n",
    "Create a new DataFrame `df_final` for clients who had a past due status, ensuring only one row per `ID`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows in df_final: 1830\n"
     ]
    }
   ],
   "source": [
    "# Filter df to include only the IDs in list_of_past_due\n",
    "df_final = df[df['ID'].isin(list_of_past_due)].drop_duplicates(subset='ID')\n",
    "\n",
    "# Number of rows in df_final\n",
    "print(f\"Number of rows in df_final: {df_final.shape[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. **Add a New Column `y = 1`**\n",
    "\n",
    "Add a new column `y` with the value `1` for all rows in `df_final`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add y column\n",
    "df_final['y'] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. **Increase `df_final` to 4,500 Rows**\n",
    "\n",
    "We need to add more rows from `df` with IDs that are not in `list_of_past_due`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter for IDs not in list_of_past_due\n",
    "df_remaining = df[~df['ID'].isin(list_of_past_due)].drop_duplicates(subset='ID')\n",
    "\n",
    "# Add rows to df_final to reach 4,500 rows\n",
    "df_final = pd.concat([df_final, df_remaining.iloc[:4500 - df_final.shape[0]]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. **Fill Missing Values of `y` and Remove Columns**\n",
    "\n",
    "Fill missing `y` values with `0`, and remove the `STATUS` and `MONTHS_BALANCE` columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill missing y values with 0\n",
    "df_final.fillna({'y': 0}, inplace=True)\n",
    "\n",
    "# Remove STATUS and MONTHS_BALANCE columns\n",
    "df_final.drop(columns=['STATUS', 'MONTHS_BALANCE'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Question 3**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **Delete `ID` Column and Reset Index**\n",
    "\n",
    "Remove the `ID` column from `df_final` and reset the index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete ID column and reset index\n",
    "df_final = df_final.drop(columns=['ID']).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. **Determine Numeric and Nominal Variables**\n",
    "\n",
    "Analyzing the variables:\n",
    "\n",
    "**Numeric Variables**:\n",
    "   Numeric variables are those that represent quantitative measures and can be used for mathematical calculations. In the context of `df_final`, these typically include:\n",
    "   - **AMT_INCOME**: Annual income (numeric value)\n",
    "   - **CNT_CHILDREN**: Number of children (integer value)\n",
    "   - **DAYS_BIRTH**: Age in days (integer value)\n",
    "   - **DAYS_EMPLOYED**: Employment duration in days (integer value)\n",
    "   - **FLAG_MOBIL**: Mobile phone ownership (binary indicator, numeric)\n",
    "   - **FLAG_WORK_PHONE**: Work phone ownership (binary indicator, numeric)\n",
    "   - **FLAG_PHONE**: Landline phone ownership (binary indicator, numeric)\n",
    "   - **FLAG_EMAIL**: Email ownership (binary indicator, numeric)\n",
    "\n",
    "**Ordinal Variables**:\n",
    "   Ordinal variables represent categories with a meaningful order but not necessarily a uniform scale between them. In `df_final`, the only ordinal variable is:\n",
    "   - **NAME_EDUCATION_TYPE**: Education level (e.g., Secondary, Higher, etc., which have a meaningful order)\n",
    "\n",
    "**Nominal Variables**:\n",
    "   Nominal variables represent categories without a meaningful order. These are purely categorical and are used to label data. In `df_final`, nominal variables include:\n",
    "   - **CODE_GENDER**: Gender (e.g., Male, Female)\n",
    "   - **FLAG_OWN_CAR**: Car ownership status (e.g., Yes, No)\n",
    "   - **FLAG_OWN_REALTY**: Real estate ownership status (e.g., Yes, No)\n",
    "   - **NAME_INCOME_TYPE**: Source of income (e.g., Commercial associate, Working)\n",
    "   - **NAME_FAMILY_STATUS**: Family status (e.g., Single, Married)\n",
    "   - **NAME_HOUSING_TYPE**: Housing type (e.g., Apartment, House)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Completing the table:\n",
    "\n",
    "|Variable type|Numbers of features|Features' list|\n",
    "| --- | --- | --- |\n",
    "|Numeric:|8| AMT_INCOME, CNT_CHILDREN, DAYS_BIRTH, DAYS_EMPLOYED, FLAG_MOBIL, FLAG_WORK_PHONE, FLAG_PHONE, FLAG_EMAIL |\n",
    "|Ordinal:|1| NAME_EDUCATION_TYPE |\n",
    "|Nominal:|6| CODE_GENDER, FLAG_OWN_CAR, FLAG_OWN_REALTY, NAME_INCOME_TYPE, NAME_FAMILY_STATUS, NAME_HOUSING_TYPE |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. **Find and Comment on Missing Values**\n",
    "\n",
    "Finally, we can check for missing values in `df_final`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CODE_GENDER               0\n",
      "FLAG_OWN_CAR              0\n",
      "FLAG_OWN_REALTY           0\n",
      "CNT_CHILDREN             74\n",
      "AMT_INCOME                0\n",
      "NAME_INCOME_TYPE          0\n",
      "NAME_EDUCATION_TYPE    1831\n",
      "NAME_FAMILY_STATUS        0\n",
      "NAME_HOUSING_TYPE         0\n",
      "DAYS_BIRTH                0\n",
      "DAYS_EMPLOYED             0\n",
      "FLAG_MOBIL                0\n",
      "FLAG_WORK_PHONE           0\n",
      "FLAG_PHONE                0\n",
      "FLAG_EMAIL                0\n",
      "OCCUPATION_TYPE        1351\n",
      "CNT_FAM_MEMBERS           0\n",
      "y                         0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Check for missing values\n",
    "missing_values = df_final.isnull().sum()\n",
    "\n",
    "# Print missing values summary\n",
    "print(missing_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Variables with Missing Values:**\n",
    "   - **`CNT_CHILDREN`:** 74 missing values\n",
    "   - **`NAME_EDUCATION_TYPE`:** 1,831 missing values\n",
    "   - **`OCCUPATION_TYPE`:** 1,351 missing values\n",
    "\n",
    "**Variables with No Missing Values:**\n",
    "   - **`CODE_GENDER`, `FLAG_OWN_CAR`, `FLAG_OWN_REALTY`, `AMT_INCOME`, `NAME_INCOME_TYPE`, `NAME_FAMILY_STATUS`, `NAME_HOUSING_TYPE`, `DAYS_BIRTH`, `DAYS_EMPLOYED`, `FLAG_MOBIL`, `FLAG_WORK_PHONE`, `FLAG_PHONE`, `FLAG_EMAIL`, `CNT_FAM_MEMBERS`, `y`** have no missing values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Task 2: Data Preprocessing**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Question 4: Imputing missing values**\n",
    "\n",
    "We'll impute missing values in `df_final` considering the type of each variable and best practices. Here's how we'll approach this:\n",
    "\n",
    "1. For numeric variables, we'll use the median to impute missing values.\n",
    "\n",
    "2. For categorical variables, we'll use the mode (most frequent value) to impute missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Impute numeric variables with median\n",
    "numeric_columns = ['AMT_INCOME', 'CNT_CHILDREN', 'DAYS_BIRTH', 'DAYS_EMPLOYED', 'FLAG_MOBIL', 'FLAG_WORK_PHONE', 'FLAG_PHONE', 'FLAG_EMAIL', 'CNT_FAM_MEMBERS']\n",
    "for col in numeric_columns:\n",
    "    df_final[col] = df_final[col].fillna(df_final[col].median())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Impute categorical variables with mode\n",
    "categorical_columns = ['CODE_GENDER', 'FLAG_OWN_CAR', 'FLAG_OWN_REALTY', 'NAME_INCOME_TYPE', 'NAME_EDUCATION_TYPE', 'NAME_FAMILY_STATUS', 'NAME_HOUSING_TYPE', 'OCCUPATION_TYPE']\n",
    "for col in categorical_columns:\n",
    "    df_final[col] = df_final[col].fillna(df_final[col].mode()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CODE_GENDER            0\n",
      "FLAG_OWN_CAR           0\n",
      "FLAG_OWN_REALTY        0\n",
      "CNT_CHILDREN           0\n",
      "AMT_INCOME             0\n",
      "NAME_INCOME_TYPE       0\n",
      "NAME_EDUCATION_TYPE    0\n",
      "NAME_FAMILY_STATUS     0\n",
      "NAME_HOUSING_TYPE      0\n",
      "DAYS_BIRTH             0\n",
      "DAYS_EMPLOYED          0\n",
      "FLAG_MOBIL             0\n",
      "FLAG_WORK_PHONE        0\n",
      "FLAG_PHONE             0\n",
      "FLAG_EMAIL             0\n",
      "OCCUPATION_TYPE        0\n",
      "CNT_FAM_MEMBERS        0\n",
      "y                      0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Verify that all missing values have been imputed\n",
    "print(df_final.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Question 5: Converting values in NAME_EDUCATION_TYPE**\n",
    "\n",
    "We need to convert the values in the `NAME_EDUCATION_TYPE` column to numeric values. We'll use the following mapping:\n",
    "- Lower secondary -> 1\n",
    "- Secondary / secondary special -> 2\n",
    "- Incomplete higher -> 3\n",
    "- Higher education -> 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the mapping for education types\n",
    "education_mapping = {\n",
    "    'Lower secondary': 1,\n",
    "    'Secondary / secondary special': 2,\n",
    "    'Incomplete higher': 3,\n",
    "    'Higher education': 4\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the mapping to the NAME_EDUCATION_TYPE column\n",
    "df_final['NAME_EDUCATION_TYPE'] = df_final['NAME_EDUCATION_TYPE'].map(education_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME_EDUCATION_TYPE\n",
      "2    3610\n",
      "4     725\n",
      "3     150\n",
      "1      15\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Verify the transformation\n",
    "print(df_final['NAME_EDUCATION_TYPE'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Question 6: Adding dummy variables for nominal features**\n",
    "\n",
    "We'll create dummy variables for the nominal features in `df_final` to convert them into a format suitable for machine learning models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of nominal features\n",
    "nominal_features = ['CODE_GENDER', 'FLAG_OWN_CAR', 'FLAG_OWN_REALTY', 'NAME_INCOME_TYPE', 'NAME_FAMILY_STATUS', 'NAME_HOUSING_TYPE', 'OCCUPATION_TYPE']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dummy variables\n",
    "df_dummies = pd.get_dummies(df_final[nominal_features], drop_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove original nominal features and add dummy variables\n",
    "df_final = df_final.drop(columns=nominal_features).join(df_dummies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['CNT_CHILDREN', 'AMT_INCOME', 'NAME_EDUCATION_TYPE', 'DAYS_BIRTH',\n",
      "       'DAYS_EMPLOYED', 'FLAG_MOBIL', 'FLAG_WORK_PHONE', 'FLAG_PHONE',\n",
      "       'FLAG_EMAIL', 'CNT_FAM_MEMBERS', 'y', 'CODE_GENDER_M', 'FLAG_OWN_CAR_Y',\n",
      "       'FLAG_OWN_REALTY_Y', 'NAME_INCOME_TYPE_Pensioner',\n",
      "       'NAME_INCOME_TYPE_State servant', 'NAME_INCOME_TYPE_Student',\n",
      "       'NAME_INCOME_TYPE_Working', 'NAME_FAMILY_STATUS_Married',\n",
      "       'NAME_FAMILY_STATUS_Separated',\n",
      "       'NAME_FAMILY_STATUS_Single / not married', 'NAME_FAMILY_STATUS_Widow',\n",
      "       'NAME_HOUSING_TYPE_House / apartment',\n",
      "       'NAME_HOUSING_TYPE_Municipal apartment',\n",
      "       'NAME_HOUSING_TYPE_Office apartment',\n",
      "       'NAME_HOUSING_TYPE_Rented apartment', 'NAME_HOUSING_TYPE_With parents',\n",
      "       'OCCUPATION_TYPE_Cleaning staff', 'OCCUPATION_TYPE_Cooking staff',\n",
      "       'OCCUPATION_TYPE_Core staff', 'OCCUPATION_TYPE_Drivers',\n",
      "       'OCCUPATION_TYPE_HR staff', 'OCCUPATION_TYPE_High skill tech staff',\n",
      "       'OCCUPATION_TYPE_IT staff', 'OCCUPATION_TYPE_Laborers',\n",
      "       'OCCUPATION_TYPE_Low-skill Laborers', 'OCCUPATION_TYPE_Managers',\n",
      "       'OCCUPATION_TYPE_Medicine staff',\n",
      "       'OCCUPATION_TYPE_Private service staff',\n",
      "       'OCCUPATION_TYPE_Realty agents', 'OCCUPATION_TYPE_Sales staff',\n",
      "       'OCCUPATION_TYPE_Secretaries', 'OCCUPATION_TYPE_Security staff',\n",
      "       'OCCUPATION_TYPE_Waiters/barmen staff'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# Verify the changes\n",
    "print(df_final.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Task 3: Preparing Data for Modeling**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Question 7**\n",
    "\n",
    "**1. Creating the y array**\n",
    "\n",
    "- We use `df_final['y']` to select the 'y' column from our DataFrame.\n",
    "\n",
    "- `.to_numpy()` converts this pandas Series to a NumPy array.\n",
    "\n",
    "- `dtype=int` ensures that the values are stored as integers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create y array from the 'y' column of df_final\n",
    "y = df_final['y'].to_numpy(dtype=int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. Creating the `X` array**\n",
    "- `df_final.drop('y', axis=1)` creates a new DataFrame without the 'y' column.\n",
    "\n",
    "- `.to_numpy()` converts this DataFrame to a NumPy array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create X array from all remaining features in df_final\n",
    "X = df_final.drop('y', axis=1).to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Question 8**\n",
    "\n",
    "**1. Splitting the data into train and test sets**\n",
    "\n",
    "- We use `train_test_split` from scikit-learn to split our data.\n",
    "\n",
    "- `test_size=0.25` ensures 75% train and 25% test split.\n",
    "\n",
    "- `random_state=8` sets a seed for reproducibility.\n",
    "\n",
    "- `stratify=y` ensures that the proportion of samples for each class is roughly equal in both train and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into train and test sets\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=8, stratify=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. Standardizing the data**\n",
    "- We use StandardScaler from scikit-learn to standardize our data.\n",
    "\n",
    "- fit_transform on X_train both computes the mean and std to be used for later scaling, and scales the training data.\n",
    "\n",
    "- We use transform on X_test to scale it using the mean and std computed from X_train. This is crucial to prevent data leakage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the StandardScaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit the scaler on the training data and transform both training and test data\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Task 4: Model Training and Evaluation**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Question 9**\n",
    "\n",
    "1. **Logistic Regression Classifier**:\n",
    "   - Train the Logistic Regression model on the standardized data.\n",
    "   - Calculate and print the training and test accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Results:\n",
      "Training Accuracy: 0.655\n",
      "Test Accuracy: 0.656\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Initialize the Logistic Regression model\n",
    "lr_model = LogisticRegression(random_state=10)\n",
    "\n",
    "# Train the model on the standardized training data\n",
    "lr_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Make predictions on both training and test sets\n",
    "lr_train_pred = lr_model.predict(X_train_scaled)\n",
    "lr_test_pred = lr_model.predict(X_test_scaled)\n",
    "\n",
    "# Compute accuracies\n",
    "lr_train_accuracy = accuracy_score(y_train, lr_train_pred)\n",
    "lr_test_accuracy = accuracy_score(y_test, lr_test_pred)\n",
    "\n",
    "# Print accuracies rounded to three decimal places\n",
    "print(\"Logistic Regression Results:\")\n",
    "print(f\"Training Accuracy: {lr_train_accuracy:.3f}\")\n",
    "print(f\"Test Accuracy: {lr_test_accuracy:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. **Random Forest Classifier**:\n",
    "   - Train the Random Forest model on the standardized data.\n",
    "   - Calculate and print the training and test accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Random Forest Results:\n",
      "Training Accuracy: 0.975\n",
      "Test Accuracy: 0.903\n"
     ]
    }
   ],
   "source": [
    "# Initialize the Random Forest model\n",
    "rf_model = RandomForestClassifier(random_state=10)\n",
    "\n",
    "# Train the model on the standardized training data\n",
    "rf_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Make predictions on both training and test sets\n",
    "rf_train_pred = rf_model.predict(X_train_scaled)\n",
    "rf_test_pred = rf_model.predict(X_test_scaled)\n",
    "\n",
    "# Compute accuracies\n",
    "rf_train_accuracy = accuracy_score(y_train, rf_train_pred)\n",
    "rf_test_accuracy = accuracy_score(y_test, rf_test_pred)\n",
    "\n",
    "# Print accuracies rounded to three decimal places\n",
    "print(\"\\nRandom Forest Results:\")\n",
    "print(f\"Training Accuracy: {rf_train_accuracy:.3f}\")\n",
    "print(f\"Test Accuracy: {rf_test_accuracy:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Question 9**\n",
    "\n",
    "3. **Model Comparison and Insights**\n",
    "\n",
    "a. **Comparing training and test accuracies for each classifier**:\n",
    "\n",
    "**Logistic Regression**:\n",
    "- **Training Accuracy**: 0.655\n",
    "- **Test Accuracy**: 0.656\n",
    "\n",
    "The Logistic Regression model shows very similar accuracies for both training and test sets, with only a 0.001 difference. This suggests that the model is not overfitting. It has learned patterns that generalize well to unseen data, but its overall performance is moderate.\n",
    "\n",
    "**Random Forest**:\n",
    "- **Training Accuracy**: 0.975\n",
    "- **Test Accuracy**: 0.903\n",
    "\n",
    "The Random Forest model shows a higher training accuracy (0.975) compared to its test accuracy (0.903). This difference of 0.072 indicates some degree of overfitting. The model has learned the training data very well, including some patterns that don't generalize to the test set.\n",
    "\n",
    "**Extent of overfitting**:\n",
    "- Logistic Regression: Minimal to no overfitting. The model's performance on training and test data is nearly identical.\n",
    "- Random Forest: Moderate overfitting. The model performs notably better on the training data than on the test data, suggesting it has learned some patterns specific to the training set that don't generalize well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b. **Comparing accuracies across the two classifiers**:\n",
    "\n",
    "The Random Forest classifier provides better forecasts overall:\n",
    "- **Logistic Regression Test Accuracy**: 0.656\n",
    "- **Random Forest Test Accuracy**: 0.903\n",
    "\n",
    "The Random Forest outperforms the Logistic Regression by a significant margin (0.247 or 24.7 percentage points) on the test set. This suggests that the Random Forest is much better at capturing the underlying patterns in the data that are relevant for prediction.\n",
    "\n",
    "Despite its slight overfitting, the Random Forest still generalizes much better to unseen data compared to the Logistic Regression model. Its test accuracy of 0.903 indicates strong predictive performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c. **Presence of nonlinearities in the dataset**:\n",
    "\n",
    "The significant performance difference between the Logistic Regression (a linear model) and the Random Forest (a nonlinear model) strongly suggests the presence of nonlinearities in the credit default prediction dataset. Here's a detailed explanation considering the context:\n",
    "\n",
    "1. **Nature of Credit Default Prediction**:\n",
    "   Credit default prediction is inherently complex, often involving nonlinear relationships between various factors. For instance, the relationship between income (`AMT_INCOME`) and default risk may not be linear - there could be threshold effects or diminishing returns.\n",
    "\n",
    "2. **Diverse Feature Set**:\n",
    "   The dataset includes a wide range of features such as income, education level, employment duration, family status, and various binary flags (car ownership, phone ownership, etc.). The interaction between these diverse features is likely to be nonlinear. For example, the effect of income on default risk might vary depending on education level or family status.\n",
    "\n",
    "3. **Temporal Aspects**:\n",
    "   The `DAYS_BIRTH` and 'DAYS_EMPLOYED' features count backwards from the current day. The impact of age or employment duration on credit risk may not be linear - there could be critical periods or thresholds that significantly affect risk.\n",
    "\n",
    "4. **Categorical Variables**:\n",
    "   The dataset includes several categorical variables (`NAME_INCOME_TYPE`, `NAME_EDUCATION_TYPE`, `OCCUPATION_TYPE`). The Random Forest's superior performance suggests it's better at handling the potentially nonlinear effects of these categorical variables on credit risk.\n",
    "\n",
    "5. **Credit History Complexity**:\n",
    "   The `STATUS` variable in the `credit_record.csv` file indicates various levels of payment delays. The relationship between past payment behavior and future default risk is likely to be nonlinear, with recent behavior possibly having a disproportionate impact.\n",
    "\n",
    "6. **Model Performance Comparison**:\n",
    "   The Logistic Regression model, which assumes linear relationships, achieved only moderate accuracy (0.656). In contrast, the Random Forest model, capable of capturing nonlinear patterns, achieved much higher accuracy (0.903). This substantial improvement (24.7 percentage points) strongly indicates that nonlinear relationships in the data are crucial for predicting credit defaults.\n",
    "\n",
    "7. **Feature Interactions**:\n",
    "   In credit risk assessment, the interaction between features can be critical. For instance, the impact of income on default risk might depend on the number of dependents (`CNT_CHILDREN`) or housing type (`NAME_HOUSING_TYPE`). Random Forests can automatically capture such interactions, while Logistic Regression cannot without explicit feature engineering.\n",
    "\n",
    "8. **Threshold Effects**:\n",
    "   Credit behavior often exhibits threshold effects. For example, missing a payment by a few days might have a disproportionate impact on future default risk. The Random Forest's ability to create complex decision boundaries allows it to capture such nonlinear threshold effects more effectively than Logistic Regression.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Conclusion**\n",
    "\n",
    "1. **Summary of Findings**:\n",
    "Throughout this project, we gained several key insights into predicting credit defaults for a financial institution:\n",
    "\n",
    "- **Data Integration**: By merging application records with credit histories, we created a comprehensive dataset that captures both client characteristics and their credit behavior over time.\n",
    "\n",
    "- **Data Preprocessing**: We addressed missing values in key fields like education type and number of children, ensuring a complete dataset for analysis. We also converted categorical variables into dummy variables, making them suitable for machine learning models.\n",
    "\n",
    "- **Feature Importance**: The project highlighted the significance of various factors in predicting credit defaults, including income, employment duration, age, family status, and past credit behavior.\n",
    "\n",
    "- **Model Performanc**e: We compared two models:\n",
    "  - Logistic Regression achieved accuracies of 0.655 (training) and 0.656 (test)\n",
    "  - **Random Forest** achieved accuracies of 0.975 (training) and 0.903 (test)\n",
    "\n",
    "The Random Forest model emerged as the most effective, outperforming Logistic Regression by a significant margin (24.7 percentage points on the test set). This implies that credit default prediction in this dataset involves complex, nonlinear relationships that the Random Forest can capture more effectively.\n",
    "\n",
    "The high accuracy of the **Random Forest model** (90.3% on unseen data) suggests it could be a valuable tool for the financial institution in assessing credit risk and making lending decisions. However, the slight overfitting observed (97.5% training accuracy vs 90.3% test accuracy) indicates there's room for model refinement.\n",
    "\n",
    "\n",
    "2. **Future Work**:\n",
    "To further improve this analysis and its applications, we suggest:\n",
    "\n",
    "- Feature Engineering: Create new features that might capture important aspects of credit risk, such as debt-to-income ratio or credit utilization rate.\n",
    "\n",
    "- Time Series Analysis: Incorporate more sophisticated analysis of the temporal aspects of credit behavior, possibly using time series models.\n",
    "\n",
    "- Interpretability: While Random Forest performs well, its decisions can be hard to interpret. Implementing techniques like SHAP (SHapley Additive exPlanations) values could provide insights into feature importance and model decisions.\n",
    "\n",
    "This analysis could be extended to similar datasets in the financial sector, such as:\n",
    "- Predicting early loan repayments, which can affect a bank's expected interest income.\n",
    "- Estimating the probability of a customer opening new accounts or services.\n",
    "- Detecting fraudulent transactions by incorporating similar machine learning techniques.\n",
    "\n",
    "By continually refining these models and expanding their applications, financial institutions can make more informed decisions, manage risk more effectively, and provide better services to their clients.\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
